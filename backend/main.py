from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from google.cloud import bigquery
import re
import logging
from openai import AzureOpenAI
import requests
from msal import ConfidentialClientApplication
from fastapi.responses import JSONResponse


# --- Configuraci√≥n general ---
PROJECT_ID = "advanced-analytics-dev-440814"
TABLE_ID = "comerciales.comerciales_cuentas"

# --- Configuraci√≥n Azure OpenAI ---
endpoint_llm = "https://prueba-diego-aa.openai.azure.com/"
deployment_llm = "gpt-4o"
api_key_llm = "9ea7440cfca84e5c82d42811be968b34"

client_llm = AzureOpenAI(
    azure_endpoint=endpoint_llm,
    api_key=api_key_llm,
    api_version="2025-01-01-preview"
)

# --- Inicializaci√≥n ---
app = FastAPI()
origins = ["*"]  # Para desarrollo. En producci√≥n, us√° ["https://tu-dominio.com"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,           # Qu√© or√≠genes est√°n permitidos
    allow_credentials=True,
    allow_methods=["*"],             # GET, POST, PUT, DELETE, etc.
    allow_headers=["*"],             # Content-Type, Authorization, etc.
)
client = bigquery.Client()
logging.basicConfig(level=logging.INFO)

# --- Funciones auxiliares ---
def normalizar_texto(texto: str) -> str:
    return re.sub(r'[^a-zA-Z0-9]', '', texto).lower()

def extraer_marca(pregunta: str) -> str:
    prompt = f"""
Tu tarea es analizar una pregunta de un usuario y extraer con precisi√≥n el nombre de la **marca o cuenta comercial** que se menciona.
Lo que tenes que hacer es solamente devolver las marcas que encuentres en los textos, ejemplo: Gafa - Samsung - BGH - electro misiones - BIOGREEN - ABBOTT - CETROGAR, FADECYA, COPCO, NEWSAN, FRAVEGA, DISE√ëOJERY, T&H TABACOS, PANALAB, RICHMOND, CHIESA, UPS, THIRD TIME, WOOPY, EMOOD, DABRA, etc. 
Devolv√© **solo** el nombre exacto de la marca, sin comillas, sin explicaciones, sin agregar nada m√°s. No devuelvas frases ni textos adicionales.

‚ö†Ô∏è Si la pregunta no menciona ninguna marca, respond√© solo: NINGUNA

üß™ Ejemplos:

Pregunta: "¬øQui√©n atiende la cuenta Natura?"
Marca: Natura

Pregunta: "Decime qui√©n es el comercial de Adidas"
Marca: Adidas

Pregunta: "Quiero saber qui√©n lleva la cuenta de Mercado Libre"
Marca: Mercado Libre

Pregunta: "¬øCu√°l es el ejecutivo asignado a DABRA?"
Marca: DABRA

Pregunta: "Hola, buen d√≠a"
Marca: NINGUNA

---

Pregunta: "{pregunta}"
Marca:
"""
    try:
        response = client_llm.chat.completions.create(
            model=deployment_llm,
            messages=[
                {"role": "system", "content": "Sos un extractor de marcas"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        marca = response.choices[0].message.content.strip()
        logging.info(f"[Azure OpenAI] Pregunta: '{pregunta}' ‚Üí Marca detectada: '{marca}'")
        if marca.upper() == "NINGUNA" or not marca:
            return ""
        return marca
    except Exception as e:
        logging.error(f"Error al usar Azure OpenAI: {e}")
        return ""

def buscar_comercial(marca: str) -> dict:
    """
    Devuelve un diccionario con ejecutivo, nombre_fantasia, razon_social y marca_detectada.
    """
    marca_norm = normalizar_texto(marca)

    # --- Primer intento: coincidencia exacta ---
    query_exacta = f"""
        WITH normalizada AS (
          SELECT
            ejecutivo,
            nombre_fantasia,
            razon_social,
            CASE
              WHEN LOWER(REGEXP_REPLACE(nombre_fantasia, r'[^a-zA-Z0-9]', '')) = @marca THEN 0
              ELSE 1
            END AS prioridad
          FROM `{TABLE_ID}`
          WHERE
            LOWER(REGEXP_REPLACE(nombre_fantasia, r'[^a-zA-Z0-9]', '')) = @marca
            OR LOWER(REGEXP_REPLACE(razon_social,   r'[^a-zA-Z0-9]', '')) = @marca
        )
        SELECT ejecutivo, nombre_fantasia, razon_social
        FROM normalizada
        ORDER BY prioridad
        LIMIT 1
    """

    try:
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("marca", "STRING", marca_norm)
            ]
        )
        result = client.query(query_exacta, job_config=job_config).result()
        rows = list(result)
        if rows:
            row = rows[0]
            return {
                "ejecutivo": row["ejecutivo"],
                "nombre_fantasia": row["nombre_fantasia"],
                "razon_social": row["razon_social"],
                "marca_detectada": row["nombre_fantasia"] or row["razon_social"]
            }
    except Exception as e:
        logging.error(f"[BigQuery] Error en intento exacto: {e}")

    # --- Segundo intento con ranking (prioridad) ---
    query_prioridad = f"""
        DECLARE marca_norm STRING DEFAULT @marca;

        WITH base AS (
          SELECT
            ejecutivo,
            nombre_fantasia,
            razon_social,
            LOWER(REGEXP_REPLACE(nombre_fantasia, r'[^a-zA-Z0-9]', '')) AS nf_norm,
            LOWER(REGEXP_REPLACE(razon_social,   r'[^a-zA-Z0-9]', '')) AS rs_norm
          FROM `{TABLE_ID}`
          WHERE
            LOWER(REGEXP_REPLACE(nombre_fantasia, r'[^a-zA-Z0-9]', '')) LIKE CONCAT('%', marca_norm, '%')
            OR LOWER(REGEXP_REPLACE(razon_social,   r'[^a-zA-Z0-9]', '')) LIKE CONCAT('%', marca_norm, '%')
        ),

        scored AS (
          SELECT
            *,
            CASE
              WHEN nf_norm = marca_norm THEN 0
              WHEN rs_norm = marca_norm THEN 1
              WHEN STARTS_WITH(nf_norm, marca_norm) THEN 2
              WHEN STARTS_WITH(rs_norm, marca_norm) THEN 3
              WHEN nf_norm LIKE CONCAT('%', marca_norm, '%') THEN 4
              WHEN rs_norm LIKE CONCAT('%', marca_norm, '%') THEN 5
              ELSE 9
            END AS prioridad,
            LEAST(LENGTH(nf_norm), LENGTH(rs_norm)) AS largo_aprox
          FROM base
        )

        SELECT ejecutivo, nombre_fantasia, razon_social
        FROM scored
        ORDER BY prioridad, largo_aprox
        LIMIT 1
    """

    try:
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("marca", "STRING", marca_norm)
            ]
        )
        result = client.query(query_prioridad, job_config=job_config).result()
        rows = list(result)
        if rows:
            row = rows[0]
            return {
                "ejecutivo": row["ejecutivo"],
                "nombre_fantasia": row["nombre_fantasia"],
                "razon_social": row["razon_social"],
                "marca_detectada": row["nombre_fantasia"] or row["razon_social"]
            }
    except Exception as e:
        logging.error(f"[BigQuery] Error en intento por prioridad: {e}")

    return {}

def generar_respuesta_llm(pregunta_usuario: str, datos: dict) -> str:
    """
    Usa Azure OpenAI para generar una respuesta c√°lida y humana en base al comercial detectado.
    """
    ejecutivo = datos["ejecutivo"]
    fantasia = datos["nombre_fantasia"]
    razon = datos["razon_social"]

    prompt = f"""
El usuario pregunt√≥: "{pregunta_usuario}"

Respond√© de manera c√°lida, breve y natural, como si fueras un humano que responde por Teams o WhatsApp interno.
Inclu√≠ el nombre del ejecutivo asignado a la cuenta, y mencion√° el nombre de fantas√≠a y raz√≥n social si est√°n disponibles.

Datos extra√≠dos:
- Ejecutivo: {ejecutivo}
- Nombre de fantas√≠a: {fantasia}
- Raz√≥n social: {razon}

No repitas textualmente el prompt. S√© humano, no rob√≥tico. Vari√° c√≥mo lo dec√≠s.
"""

    try:
        response = client_llm.chat.completions.create(
            model=deployment_llm,
            messages=[
                {"role": "system", "content": "Sos un asistente humano y c√°lido que ayuda a otros en la empresa a ubicar al comercial de una cuenta."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        logging.error(f"[LLM] Error generando respuesta con LLM: {e}")
        return f"{ejecutivo} es quien aparece asignado a la cuenta '{fantasia}' ({razon})."


def analizar_imagen(image_base64: str) -> str:
    """
    Usa Azure OpenAI para interpretar una imagen (base64) y devolver una descripci√≥n textual
    con objetos visibles, marcas, etiquetas y textos √∫tiles para inferir la marca.
    """
    payload = {
        "messages": [
            {
                "role": "system",
                "content": [
                    {
                        "type": "text",
                        "text": (
                            "Sos un asistente visual que describe im√°genes log√≠sticas. "
                            "Detect√°s objetos, marcas visibles, nombres escritos, etiquetas, logos y cualquier dato impreso o pegado. "
                            "Tu tarea es describir de forma clara lo que se ve, para ayudar a identificar una marca o remitente."
                        )
                    }
                ]
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": (
                            "Por favor, describ√≠ lo que ves en la imagen siguiente. "
                            "Inclu√≠ marcas, nombres, etiquetas, n√∫meros de env√≠o, textos visibles, c√≥digos o cualquier otro detalle relevante. "
                            "No inventes. Respond√© solo con una descripci√≥n."
                        )
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{image_base64}"
                        }
                    }
                ]
            }
        ],
        "temperature": 0.3,
        "top_p": 0.9,
        "max_tokens": 800
    }

    try:
        response = requests.post(
            endpoint_llm, headers={
                "Content-Type": "application/json",
                "api-key": api_key_llm
            }, json=payload
        )
        response.raise_for_status()
        descripcion = response.json()["choices"][0]["message"]["content"].strip()
        logging.info(f"[Visi√≥n LLM] Descripci√≥n generada: {descripcion}")
        return descripcion
    except requests.RequestException as e:
        logging.error(f"[Visi√≥n LLM] Error procesando imagen: {e}")
        return "No se pudo interpretar la imagen."



# --- Endpoint principal ---
@app.post("/")
async def consultar_comercial(request: Request):
    data = await request.json()
    pregunta = data.get("pregunta", "")

    if not pregunta:
        return {"respuesta": "No se recibi√≥ una pregunta v√°lida."}

    marca = extraer_marca(pregunta)
    
    # Si no hay marca, gener√° una respuesta amable igual
    if not marca:
        try:
            respuesta_llm = client_llm.chat.completions.create(
                model=deployment_llm,
                messages=[
                    {"role": "system", "content": (
                        "Sos un asistente cordial que responde de forma breve, clara y profesional. "
                        "Tu tarea es ayudar a personas de la empresa a conocer qui√©n es el ejecutivo asignado a una cuenta o marca espec√≠fica. "
                        "Si el mensaje no tiene una marca conocida, salud√° o ped√≠ que reformulen la consulta. "
                        "No prometas gestionar cuentas, facturaci√≥n, ni brindar promociones."
                        )},
                    {"role": "user", "content": pregunta}
                ],
                temperature=0.6
            )
            return {"respuesta": respuesta_llm.choices[0].message.content.strip()}
        except Exception as e:
            logging.error(f"[LLM] Error generando respuesta sin marca: {e}")
            return {"respuesta": "No entend√≠ tu mensaje, ¬øpod√©s reformularlo?"}

    # Proceso normal si hay marca detectada
    resultado = buscar_comercial(marca)
    if not resultado or not resultado.get("ejecutivo"):
        return {"respuesta": f"No se encontr√≥ un comercial para la marca '{marca}'"}

    respuesta_generada = generar_respuesta_llm(pregunta, resultado)

    return {
        "marca_detectada": resultado["marca_detectada"],
        "nombre_fantasia": resultado["nombre_fantasia"],
        "razon_social": resultado["razon_social"],
        "ejecutivo": resultado["ejecutivo"],
        "respuesta": respuesta_generada
    }



@app.post("/consulta-con-imagen")
async def consulta_con_imagen(request: Request):
    data = await request.json()
    comentario = data.get("comentario", "")
    imagen_base64 = data.get("imagen", "")

    if not comentario and not imagen_base64:
        return {"respuesta": "Faltan datos para procesar la consulta."}

    descripcion = analizar_imagen(imagen_base64)  # üëà Ac√° obten√©s el texto

    marca = extraer_marca(descripcion)
    if not marca:
        return {
            "respuesta": "No se detect√≥ ninguna marca en la imagen enviada.",
            "descripcion_imagen": descripcion  # üëà Agregalo ac√° para inspecci√≥n
        }

    resultado = buscar_comercial(marca)
    if not resultado or not resultado.get("ejecutivo"):
        return {
            "respuesta": f"No se encontr√≥ un comercial para la marca detectada: '{marca}'",
            "descripcion_imagen": descripcion
        }

    respuesta = generar_respuesta_llm(comentario, resultado)

    return {
        "marca_detectada": resultado["marca_detectada"],
        "nombre_fantasia": resultado["nombre_fantasia"],
        "razon_social": resultado["razon_social"],
        "ejecutivo": resultado["ejecutivo"],
        "descripcion_imagen": descripcion,  # üëà ac√° tambi√©n
        "respuesta": respuesta
    }



# --- Health check ---
@app.get("/")
async def root():
    return {"estado": "ok"}

@app.options("/{path:path}")
async def preflight_handler(path: str, request: Request):
    return JSONResponse(status_code=200, content={})

